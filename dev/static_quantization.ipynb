{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30db359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "from onnxruntime.quantization import CalibrationDataReader, quantize_static\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from streamsad.feature_extractor import FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2d24e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:07<00:00, 15.61it/s]\n",
      "100%|██████████| 1300/1300 [00:13<00:00, 97.52it/s]\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # data path\n",
    "    sad_base_path = \"/home/aj/mahsan/AVA/cropped_wave\"\n",
    "    sad_json_path = \"/home/aj/repo/SAD-AVA/ava_labels_exsiting_files.json\"\n",
    "    noise_base_path = \"/home/aj/additive_noise\"\n",
    "    noise_json_path = \"/home/aj/repo/SAD-AVA/music_singking_files.json\"\n",
    "\n",
    "    # features\n",
    "    duration = 60.\n",
    "    fs = 16000\n",
    "    n_fft = 512\n",
    "    n_hop = 512\n",
    "    feature_epsilon = 1e-6\n",
    "\n",
    "    # augmentation\n",
    "    augment_prob = 0.4\n",
    "    min_amplitude_percent = 10\n",
    "    max_amplitude_percent = 100\n",
    "\n",
    "\n",
    "class RandomCrop:\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.num_samples = int(Config.fs * Config.duration)\n",
    "\n",
    "    def crop(self, x: torch.Tensor) -> tuple[torch.Tensor, int]:\n",
    "        _, samples = x.size()\n",
    "        offset = torch.randint(0, samples - self.num_samples - 1, (1,))\n",
    "        return x[:, offset : offset + self.num_samples], offset.item()\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_epsilon = Config.feature_epsilon\n",
    "        self.window = np.hanning(Config.n_fft)\n",
    "\n",
    "    def compute_fft(self, x_np):\n",
    "        num_frames = x_np.shape[0] // Config.n_fft\n",
    "        fft_frames_real = []\n",
    "        for i in range(num_frames):\n",
    "            start_idx = i * Config.n_fft\n",
    "            end_idx = start_idx + Config.n_fft\n",
    "            frame = x_np[start_idx:end_idx] * self.window\n",
    "            fft_frame = np.fft.rfft(frame)\n",
    "            fft_frame = (fft_frame * fft_frame.conj()).real\n",
    "            fft_frame_real = np.log10(np.abs(fft_frame) + Config.feature_epsilon)\n",
    "            fft_frames_real.append(fft_frame_real)\n",
    "        fft_frames_real = np.array(fft_frames_real).T\n",
    "        return fft_frames_real\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x of the shape 1xT\n",
    "        x_np = x.view(-1).numpy()\n",
    "        fft_frames_real = self.compute_fft(x_np)\n",
    "        return torch.from_numpy(fft_frames_real).unsqueeze(0).float()\n",
    "\n",
    "\n",
    "class AVADS(Dataset):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.sad_base_path = Path(self.config.sad_base_path)\n",
    "        with open(config.sad_json_path) as f:\n",
    "            self.sad = json.load(f)\n",
    "        self.sad_wavs = [\n",
    "            torchaudio.load(self.sad_base_path / i[\"filename\"])[0]\n",
    "            for i in tqdm(self.sad)\n",
    "        ]\n",
    "        self.noise_base_path = Path(self.config.noise_base_path)\n",
    "        with open(config.noise_json_path) as f:\n",
    "            self.noise = json.load(f)\n",
    "        self.noise_wavs = [\n",
    "            torchaudio.load(self.noise_base_path / i)[0] for i in tqdm(self.noise)\n",
    "        ]\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.random_crop = RandomCrop()\n",
    "\n",
    "    def _has_overlap(\n",
    "        self, interval0: tuple[float, float], interval1: tuple[float, float]\n",
    "    ) -> int:\n",
    "        if interval1[\"start\"] < interval0[\"start\"] < interval1[\"end\"]:\n",
    "            return 1\n",
    "        if interval1[\"start\"] < interval0[\"end\"] < interval1[\"end\"]:\n",
    "            return 1\n",
    "        if interval0[\"start\"] < interval1[\"start\"] < interval0[\"end\"]:\n",
    "            return 1\n",
    "        if interval0[\"start\"] < interval1[\"start\"] < interval0[\"end\"]:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def _create_label(self, labels: dict, offset: int, num_samples: int) -> list[int]:\n",
    "        win_len = self.config.n_fft\n",
    "        hop_len = self.config.n_fft\n",
    "        fs = self.config.fs\n",
    "        frame_start = offset\n",
    "        frame_end = offset + win_len\n",
    "        labels_gen = iter(labels)\n",
    "        try:\n",
    "            label = next(labels_gen)\n",
    "        except StopIteration:\n",
    "            label = {\"start\": torch.inf, \"end\": torch.inf}\n",
    "        result = []\n",
    "        while frame_end <= offset + num_samples:\n",
    "            # update label if needed\n",
    "            if frame_start / fs > label[\"end\"]:\n",
    "                try:\n",
    "                    label = next(labels_gen)\n",
    "                    continue\n",
    "                except StopIteration:\n",
    "                    pass\n",
    "            # tag the frame\n",
    "            interval0 = {\"start\": frame_start / fs, \"end\": frame_end / fs}\n",
    "            result.append(self._has_overlap(interval0, label))\n",
    "            # step forward\n",
    "            frame_start += hop_len\n",
    "            frame_end += hop_len\n",
    "        return result\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sad)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.FloatTensor, torch.LongTensor]:\n",
    "        x = self.sad_wavs[index]\n",
    "        sample = self.sad[index]\n",
    "        labels = sample[\"vad\"]\n",
    "        # input\n",
    "        x, offset = self.random_crop.crop(x)\n",
    "        # amplitude augmentation\n",
    "        x *= random.uniform(0.1, 2)\n",
    "        if torch.rand(1).item() <= self.config.augment_prob:\n",
    "            noise = random.choice(self.noise_wavs)\n",
    "            # print(f\"1: {noise.shape = }\")\n",
    "            # print(f\"{x.shape = }\")\n",
    "            while noise.size(1) <= x.size(1):\n",
    "                noise = torch.cat([noise, noise], dim=1)\n",
    "            # print(f\"2: {noise.shape = }\")\n",
    "            noise, _ = self.random_crop.crop(noise)\n",
    "            noise *= (\n",
    "                torch.randint(\n",
    "                    self.config.min_amplitude_percent,\n",
    "                    self.config.max_amplitude_percent,\n",
    "                    size=(1,),\n",
    "                )\n",
    "                / 100\n",
    "            )\n",
    "            x += noise\n",
    "        num_samples = x.size(1)\n",
    "        lmfb = self.feature_extractor(x).squeeze(0)\n",
    "        # target\n",
    "        target = self._create_label(labels, offset, num_samples)\n",
    "        # check size\n",
    "        while len(target) < lmfb.size(1):\n",
    "            target.append(target[-1])\n",
    "        target = torch.LongTensor(target)\n",
    "        return x, lmfb, target\n",
    "\n",
    "\n",
    "sad_dataset = AVADS(Config)\n",
    "sad_dataloader = DataLoader(sad_dataset, batch_size=1, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ccfe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SADCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.dl_iter = None\n",
    "        self.rewind()\n",
    "\n",
    "    def get_state(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return np.zeros((batch_size, 1, 64), dtype=np.float32)\n",
    "\n",
    "    def get_next(self):\n",
    "        x = next(self.dl_iter, None)\n",
    "        if x is not None:\n",
    "            # x is a tuple consists of: (audio, lmfb, target)\n",
    "            x = x[1]\n",
    "            return {\"input\": x.numpy(), \"input_state\": self.get_state(x)}\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def rewind(self):\n",
    "        self.dl_iter = iter(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "445c4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_calibration_data_reader = SADCalibrationDataReader(sad_dataloader)\n",
    "\n",
    "# input_dict = sad_calibration_data_reader.get_next()\n",
    "# model_fp32 = \"../src/streamsad/models/model_2025-06-10.onnx\"\n",
    "# ort_session_fp32 = ort.InferenceSession(model_fp32)\n",
    "# raw_output_fp32, _ = ort_session_fp32.run(\n",
    "#     None,\n",
    "#     input_dict,\n",
    "# )\n",
    "# input_dict[\"input\"].shape, input_dict[\"input_state\"].shape, input_dict.keys, input_dict[\"input\"].dtype, input_dict[\"input_state\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e98f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = \"../src/streamsad/models/model_2025-06-10.onnx\"\n",
    "model_pre = \"model_2025-06-10_pre.onnx\"\n",
    "model_int8_static = \"model_2025-06-10_static_int8.onnx\"\n",
    "\n",
    "q_static_opts = {\"ActivationSymmetric\":False, \"WeightSymmetric\":True}\n",
    "if torch.cuda.is_available():\n",
    "    q_static_opts = {\"ActivationSymmetric\":True, \"WeightSymmetric\":True}\n",
    "\n",
    "quantized_model = quantize_static(\n",
    "    model_input=model_pre,\n",
    "    model_output=model_int8_static,\n",
    "    calibration_data_reader=sad_calibration_data_reader,\n",
    "    extra_options=q_static_opts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15e84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
